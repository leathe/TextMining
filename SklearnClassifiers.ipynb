{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Classifiers combined to one (SVC classifier is not included)\n",
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "        \n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        \n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Disabled randomizing to be able to check against positive or negative data.\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for w in movie_reviews.words():\n",
    "    all_words.append(w.lower())\n",
    "    \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "# print(all_words.most_common(15))\n",
    "# print(all_words[\"movie\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SLMRD/test/ contains neg/*.txt and pos/*.txt files\n",
    "# # SLMRD test set\n",
    "# import os\n",
    "# PathNeg = \"SLMRD/test/neg/\"\n",
    "# PathPos = \"SLMRD/test/pos/\"\n",
    "# filelistNeg = os.listdir(PathNeg)\n",
    "# filelistPos = os.listdir(PathPos)\n",
    "# slmrd_negative_reviews = []\n",
    "# slmrd_positive_reviews = []\n",
    "# slmrd_test_reviews = []\n",
    "\n",
    "# for i in filelistNeg:\n",
    "#     if i.endswith(\".txt\"):\n",
    "#         with open(PathNeg + i, 'r', encoding=\"utf-8\") as f:\n",
    "#             for line in f:\n",
    "#                 slmrd_negative_reviews.append((line, 'neg'))\n",
    "#         f.close()\n",
    "\n",
    "\n",
    "# for i in filelistPos:\n",
    "#     if i.endswith(\".txt\"):\n",
    "#         with open(PathPos + i, 'r', encoding=\"utf-8\") as f:\n",
    "#             for line in f:\n",
    "#                 slmrd_positive_reviews.append((line, 'pos'))\n",
    "#         f.close()\n",
    "\n",
    "\n",
    "# # print('Negative: ', slmrd_negative_reviews[1])\n",
    "# # print('Positive: ', slmrd_positive_reviews[1])\n",
    "# for (words, sentiment) in slmrd_positive_reviews + slmrd_negative_reviews:\n",
    "#     words_filtered = [e.lower() for e in words.split() if len(e) >= 3]\n",
    "#     slmrd_test_reviews.append((words_filtered, sentiment))\n",
    "    \n",
    "# slmrd_featuresets = [(find_features(rev), category) for (rev, category) in slmrd_test_reviews]\n",
    "\n",
    "# # print('SLMRD Featureset: ', slmrd_featuresets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing_set = slmrd_featuresets\n",
    "# training_set = featuresets\n",
    "\n",
    "training_set = featuresets[:1900]\n",
    "testing_set = featuresets[1900:]\n",
    "\n",
    "# Positive data example:\n",
    "# training_set = featuresets[:1900]\n",
    "# testing_set = featuresets[1900:]\n",
    "\n",
    "# Negative data example:\n",
    "# training_set = featuresets[100:]\n",
    "# testing_set = featuresets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Classifier accuracy percent:  80.0\n"
     ]
    }
   ],
   "source": [
    "# NB_classifier = nltk.NaiveBayesClassifier()\n",
    "\n",
    "## Load an existing classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# NB_classifier_f = open(\"SavedTrainingData/NB_classifier.pickle\", \"rb\")\n",
    "# NB_classifier = pickle.load(NB_classifier_f)\n",
    "# NB_classifier_f.close()\n",
    "\n",
    "## Uncomment below to re-train the training set.\n",
    "NB_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Original Naive Bayes Classifier accuracy percent: \", (nltk.classify.accuracy(NB_classifier, testing_set))*100)\n",
    "# classifier.show_most_informative_features(15)\n",
    "\n",
    "## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# print(\"Saving training data...\")\n",
    "# save_NB_classifier = open(\"SavedTrainingData/NB_classifier.pickle\", \"wb\")\n",
    "# pickle.dump(NB_classifier, save_NB_classifier)\n",
    "# save_NB_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier accuracy percent:  82.0\n"
     ]
    }
   ],
   "source": [
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "\n",
    "## Load an existing MNB_classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# MNB_classifier_f = open(\"SavedTrainingData/MNB_classifier.pickle\", \"rb\")\n",
    "# MNB_classifier = pickle.load(MNB_classifier_f)\n",
    "# MNB_classifier_f.close()\n",
    "\n",
    "## Uncomment below to re-train the training set.\n",
    "MNB_classifier.train(training_set)\n",
    "\n",
    "print(\"MNB_classifier accuracy percent: \", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# print(\"Saving training data...\")\n",
    "# save_MNB_classifier = open(\"SavedTrainingData/MNB_classifier.pickle\", \"wb\")\n",
    "# pickle.dump(MNB_classifier, save_MNB_classifier)\n",
    "# save_MNB_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB_classifier accuracy percent: 80.0\n"
     ]
    }
   ],
   "source": [
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "\n",
    "## Load an existing BNB_classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# BNB_classifier_f = open(\"SavedTrainingData/BNB_classifier.pickle\", \"rb\")\n",
    "# BNB_classifier = pickle.load(BNB_classifier_f)\n",
    "# BNB_classifier_f.close()\n",
    "\n",
    "## Uncomment below to re-train the training set.\n",
    "BNB_classifier.train(training_set)\n",
    "\n",
    "print(\"BernoulliNB_classifier accuracy percent:\",(nltk.classify.accuracy(BNB_classifier, testing_set))*100)\n",
    "\n",
    "## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# print(\"Saving training data...\")\n",
    "# save_BNB_classifier = open(\"SavedTrainingData/BNB_classifier.pickle\", \"wb\")\n",
    "# pickle.dump(BNB_classifier, save_BNB_classifier)\n",
    "# save_BNB_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier accuracy percent: 85.0\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "\n",
    "## Load an existing LogisticRegression_classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# LogisticRegression_classifier_f = open(\"SavedTrainingData/LogisticRegression_classifier.pickle\", \"rb\")\n",
    "# LogisticRegression_classifier = pickle.load(LogisticRegression_classifier_f)\n",
    "# LogisticRegression_classifier_f.close()\n",
    "\n",
    "## Uncomment below to re-train the training set.\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", \n",
    "      (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# print(\"Saving training data...\")\n",
    "# save_LogisticRegression_classifier = open(\"SavedTrainingData/LogisticRegression_classifier.pickle\", \"wb\")\n",
    "# pickle.dump(LogisticRegression_classifier, save_LogisticRegression_classifier)\n",
    "# save_LogisticRegression_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Applications\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier_classifier accuracy percent: 87.0\n"
     ]
    }
   ],
   "source": [
    "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "\n",
    "## Load an existing SGDClassifier_classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# SGDClassifier_classifier_f = open(\"SavedTrainingData/SGDClassifier_classifier.pickle\", \"rb\")\n",
    "# SGDClassifier_classifier = pickle.load(SGDClassifier_classifier_f)\n",
    "# SGDClassifier_classifier_f.close()\n",
    "\n",
    "## Uncomment below to re-train the training set.\n",
    "SGDClassifier_classifier.train(training_set)\n",
    "\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# print(\"Saving training data...\")\n",
    "# save_SGDClassifier_classifier = open(\"SavedTrainingData/SGDClassifier_classifier.pickle\", \"wb\")\n",
    "# pickle.dump(SGDClassifier_classifier, save_SGDClassifier_classifier)\n",
    "# save_SGDClassifier_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC_classifier = SklearnClassifier(SVC())\n",
    "\n",
    "# ## Load an existing SVC_classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# SVC_classifier_f = open(\"SavedTrainingData/SVC_classifier.pickle\", \"rb\")\n",
    "# SVC_classifier = pickle.load(SVC_classifier_f)\n",
    "# SVC_classifier_f.close()\n",
    "\n",
    "# ## Uncomment below to re-train the training set.\n",
    "# # SVC_classifier.train(training_set)\n",
    "\n",
    "# print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_classifier, testing_set))*100)\n",
    "\n",
    "# ## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# # print(\"Saving training data...\")\n",
    "# # save_SVC_classifier = open(\"SavedTrainingData/SVC_classifier.pickle\", \"wb\")\n",
    "# # pickle.dump(SVC_classifier, save_SVC_classifier)\n",
    "# # save_SVC_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy percent: 84.0\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "\n",
    "## Load an existing LinearSVC_classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# LinearSVC_classifier_f = open(\"SavedTrainingData/LinearSVC_classifier.pickle\", \"rb\")\n",
    "# LinearSVC_classifier = pickle.load(LinearSVC_classifier_f)\n",
    "# LinearSVC_classifier_f.close()\n",
    "\n",
    "## Uncomment below to re-train the training set.\n",
    "LinearSVC_classifier.train(training_set)\n",
    "\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# print(\"Saving training data...\")\n",
    "# save_LinearSVC_classifier = open(\"SavedTrainingData/LinearSVC_classifier.pickle\", \"wb\")\n",
    "# pickle.dump(LinearSVC_classifier, save_LinearSVC_classifier)\n",
    "# save_LinearSVC_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NuSVC_classifier accuracy percent: 85.0\n"
     ]
    }
   ],
   "source": [
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "\n",
    "## Load an existing NuSVC_classifier.pickle file to the classifier, instead of re-training it every time.\n",
    "# NuSVC_classifier_f = open(\"SavedTrainingData/NuSVC_classifier.pickle\", \"rb\")\n",
    "# NuSVC_classifier = pickle.load(NuSVC_classifier_f)\n",
    "# NuSVC_classifier_f.close()\n",
    "\n",
    "## Uncomment below to re-train the training set.\n",
    "NuSVC_classifier.train(training_set)\n",
    "\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "## Save the classifier as a .pickle file. Uncomment below to save the trained data.\n",
    "# print(\"Saving training data...\")\n",
    "# save_NuSVC_classifier = open(\"SavedTrainingData/NuSVC_classifier.pickle\", \"wb\")\n",
    "# pickle.dump(NuSVC_classifier, save_NuSVC_classifier)\n",
    "# save_NuSVC_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifiers accuracy percent: 88.0\n",
      "Classification: neg Confidence %: 85.71428571428571\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: neg Confidence %: 100.0\n",
      "Classification: pos Confidence %: 57.14285714285714\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-fe691e7bff2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Classification:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Confidence %:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Classification:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Confidence %:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Classification:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m281\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Confidence %:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m281\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Classification:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1259\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Confidence %:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoted_classifiers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1259\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "voted_classifiers = VoteClassifier(NB_classifier, \n",
    "                                   MNB_classifier, \n",
    "                                   BNB_classifier, \n",
    "                                   LogisticRegression_classifier, \n",
    "                                   SGDClassifier_classifier, \n",
    "                                   LinearSVC_classifier, \n",
    "                                   NuSVC_classifier)\n",
    "print(\"voted_classifiers accuracy percent:\", (nltk.classify.accuracy(voted_classifiers, testing_set))*100)\n",
    "\n",
    "print(\"Classification:\", voted_classifiers.classify(testing_set[0][0]), \"Confidence %:\", voted_classifiers.confidence(testing_set[0][0])*100)\n",
    "print(\"Classification:\", voted_classifiers.classify(testing_set[5][0]), \"Confidence %:\", voted_classifiers.confidence(testing_set[5][0])*100)\n",
    "print(\"Classification:\", voted_classifiers.classify(testing_set[17][0]), \"Confidence %:\", voted_classifiers.confidence(testing_set[17][0])*100)\n",
    "print(\"Classification:\", voted_classifiers.classify(testing_set[55][0]), \"Confidence %:\", voted_classifiers.confidence(testing_set[55][0])*100)\n",
    "print(\"Classification:\", voted_classifiers.classify(testing_set[281][0]), \"Confidence %:\", voted_classifiers.confidence(testing_set[281][0])*100)\n",
    "print(\"Classification:\", voted_classifiers.classify(testing_set[1259][0]), \"Confidence %:\", voted_classifiers.confidence(testing_set[1259][0])*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(testing_set[0][0],\" \",\n",
    "# testing_set[5][0],\" \",\n",
    "# testing_set[17][0],\" \",\n",
    "# testing_set[55][0],\" \",\n",
    "# testing_set[281][0],\" \",\n",
    "# testing_set[1259][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
