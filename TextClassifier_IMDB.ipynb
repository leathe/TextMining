{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 24,
=======
   "execution_count": 140,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 25,
=======
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
   "metadata": {},
=======
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "def get_word_features(reviews):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in reviews:\n",
    "          all_words.extend(words)\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = []\n",
    "    for feature in wordlist:\n",
    "        if wordlist[feature] > 10000:\n",
    "            word_features.append(feature)\n",
    "    print(len(word_features))\n",
    "    return dict(word_features)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 85,
   "metadata": {},
=======
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pos_reviews=[]\n",
    "train_neg_reviews=[]\n",
    "test_pos_reviews=[]\n",
    "test_neg_reviews=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from os import getcwd, chdir\n",
    "def list_files(directory, extension):\n",
    "    saved = getcwd()\n",
    "    chdir(directory)\n",
    "    it = glob('*.' + extension)\n",
    "    chdir(saved)\n",
    "    return it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "import random\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 86,
=======
   "execution_count": 148,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = list(all_words)[:50]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 149,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 88,
=======
   "execution_count": 150,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 89,
=======
   "execution_count": 151,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "0.608\n"
=======
      "Reviews filtered\n",
      "64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 4; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-3f93f494885f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Reviews filtered\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mword_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_word_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reviews\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtest_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Word Features Generated\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-143-9375c2ebe752>\u001b[0m in \u001b[0;36mget_word_features\u001b[1;34m(reviews)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mword_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: dictionary update sequence element #0 has length 4; 2 is required"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "print(nltk.classify.accuracy(classifier, test_set))"
=======
    "print(\"Reviews filtered\")\n",
    "\n",
    "word_features = get_word_features(train_reviews+test_reviews)\n",
    "print(\"Word Features Generated\")\n",
    "\n",
    "training_set = nltk.classify.apply_features(extract_features, train_reviews)\n",
    "test_set = nltk.classify.apply_features(extract_features, test_reviews)\n",
    "print(\"Training and Test Sets Created\")\n",
    "\n",
    "# NaiveBayes Classifier\n",
    "print(\"Generating NaiveBayes Classifier Model\")\n",
    "NB_classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "sorted(NB_classifier.labels())\n",
    "print(\"NaiveBayes Classifier Model generated\")\n"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 91,
=======
   "execution_count": 135,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Most Informative Features\n",
      "     contains(continues) = True              pos : neg    =      2.0 : 1.0\n",
      "         contains(sorta) = True              pos : neg    =      1.9 : 1.0\n",
      "      contains(accident) = True              pos : neg    =      1.8 : 1.0\n",
      "           contains(his) = False             neg : pos    =      1.6 : 1.0\n",
      "            contains(to) = False             neg : pos    =      1.6 : 1.0\n"
=======
      "Naive Bayes Classifier:\n",
      "Accuracy:  0.61892\n",
      "Most Informative Features\n",
      "           contains(bad) = True              neg : pos    =      2.9 : 1.0\n",
      "         contains(great) = True              pos : neg    =      1.8 : 1.0\n",
      "          contains(also) = True              pos : neg    =      1.5 : 1.0\n",
      "          contains(very) = True              pos : neg    =      1.5 : 1.0\n",
      "          contains(most) = True              pos : neg    =      1.5 : 1.0\n",
      "          contains(well) = True              pos : neg    =      1.3 : 1.0\n",
      "         contains(movie) = True              neg : pos    =      1.3 : 1.0\n",
      "           contains(his) = True              pos : neg    =      1.3 : 1.0\n",
      "          contains(will) = True              pos : neg    =      1.3 : 1.0\n",
      "         contains(story) = True              pos : neg    =      1.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Print NB Classifier\n",
    "print('Naive Bayes Classifier:')\n",
    "print('Accuracy: ', nltk.classify.accuracy(NB_classifier,test_set))\n",
    "NB_classifier.show_most_informative_features()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-59e6e4bd3c89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mSK_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSklearnClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBernoulliNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mSK_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify_many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# SK_classifier = SklearnClassifier(SVC(), sparse=False).train(training_set)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\lib\\site-packages\\nltk\\classify\\scikitlearn.py\u001b[0m in \u001b[0;36mclassify_many\u001b[1;34m(self, featuresets)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \"\"\"\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\dict_vectorizer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \"\"\"\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, X, fitting)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;31m# same time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m                     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%s%s%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseparator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Applications\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\six.py\u001b[0m in \u001b[0;36miteritems\u001b[1;34m(d, **kw)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;34m\"\"\"Return an iterator over the (key, value) pairs of a dictionary.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_iteritems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0miterlists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "classifier.show_most_informative_features(5)"
=======
    "# SklearnClassifier\n",
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SK_classifier = SklearnClassifier(BernoulliNB()).train(training_set)\n",
    "SK_classifier.classify_many(test_set)\n",
    "\n",
    "# SK_classifier = SklearnClassifier(SVC(), sparse=False).train(training_set)\n",
    "# SK_classifier.classify_many(test_set)\n",
    "\n",
    "train_data = [({\"a\": 4, \"b\": 1, \"c\": 0}, \"ham\"),\n",
    "               ({\"a\": 5, \"b\": 2, \"c\": 1}, \"ham\"),\n",
    "               ({\"a\": 0, \"b\": 3, \"c\": 4}, \"spam\"),\n",
    "               ({\"a\": 5, \"b\": 1, \"c\": 1}, \"ham\"),\n",
    "               ({\"a\": 1, \"b\": 4, \"c\": 3}, \"spam\")]\n",
    "\n",
    "test_data = [{\"a\": 3, \"b\": 2, \"c\": 1},\n",
    "              {\"a\": 0, \"b\": 3, \"c\": 7}]\n",
    "\n",
    "classif = SklearnClassifier(BernoulliNB()).train(train_data)\n",
    "classif.classify_many(test_data)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "metadata": {},
=======
   "metadata": {
    "collapsed": true
   },
>>>>>>> Stashed changes
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
